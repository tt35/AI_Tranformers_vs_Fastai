{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tt35/AI_Tranformers_vs_Fastai/blob/main/Final_Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486704fe",
      "metadata": {
        "id": "486704fe"
      },
      "source": [
        "#**Transformers vs. Fastai in image classification task - Part2**\n",
        "### Which one is better and more suitable for learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd8e387",
      "metadata": {
        "id": "6fd8e387"
      },
      "source": [
        "From Recreate Model with Transformers #2, one thing that I noticed while creating the model was how tedious the whole process was. Unlike using Fast.ai, it requires a lot of coding. Thus, I decided to make some straightforward functions that do some of the tedious work for us."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b94dfba",
      "metadata": {
        "id": "0b94dfba"
      },
      "source": [
        "## Install necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b0edb5",
      "metadata": {
        "id": "98b0edb5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, load_from_disk\n",
        "import random\n",
        "from PIL import ImageDraw, ImageFont, Image\n",
        "from transformers import ViTFeatureExtractor\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "from transformers import ViTForImageClassification\n",
        "from transformers import TrainingArguments\n",
        "from transformers import Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c967b80",
      "metadata": {
        "id": "2c967b80"
      },
      "source": [
        "### Some functions and parameters from #1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ec3a476",
      "metadata": {
        "id": "0ec3a476"
      },
      "source": [
        "Similar to the #1, I created some parameters such as `feature_extractor`, and `metric`. I also reused some of the same functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63bdf4d2",
      "metadata": {
        "id": "63bdf4d2"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5557d6ea",
      "metadata": {
        "id": "5557d6ea"
      },
      "outputs": [],
      "source": [
        "def transform(example_batch):\n",
        "    # Take a list of PIL images and turn them to pixel values\n",
        "    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')\n",
        "\n",
        "    # Don't forget to include the labels!\n",
        "    inputs['label'] = example_batch['label']\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0d894b",
      "metadata": {
        "id": "3d0d894b"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8a9a18",
      "metadata": {
        "id": "6f8a9a18"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['label'] for x in batch])\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72de744e",
      "metadata": {
        "id": "72de744e"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d73dbed0",
      "metadata": {
        "id": "d73dbed0"
      },
      "source": [
        "Here, I made a function that creates a dataset for you. It takes three parameters, which are `train_path`, `test_path`, and `valid_path`. Each path has to include subfolders for each category. (e.g. train/dogs & train/cats, test/dogs & test/cats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13203bf4",
      "metadata": {
        "id": "13203bf4"
      },
      "outputs": [],
      "source": [
        "def create_ds(train_path, test_path, valid_path):\n",
        "         ds = DatasetDict({\n",
        "            \"train\": load_dataset(\"imagefolder\", data_dir= train_path, split='train'),\n",
        "            \"test\": load_dataset(\"imagefolder\", data_dir= test_path, split='train'),\n",
        "            \"validation\": load_dataset(\"imagefolder\", data_dir= valid_path, split='train')})\n",
        "         return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fbbd39c",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "8c87d57e390549be9a0d448fb11623b3",
            "3ff89162fc3e4e6fb10f5646eb2a531a",
            "71d3c2edbff74b3cae20d0d3f4785f1e"
          ]
        },
        "id": "9fbbd39c",
        "outputId": "cecbdfcd-9861-4777-ddc0-7332df32353b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c87d57e390549be9a0d448fb11623b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/107 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-ac41a5ade1cccd36\n",
            "Reusing dataset image_folder (/scratch/cs344/huggingface/datasets/image_folder/default-ac41a5ade1cccd36/0.0.0/ee92df8e96c6907f3c851a987be3fd03d4b93b247e727b69a8e23ac94392a091)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ff89162fc3e4e6fb10f5646eb2a531a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/34 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-73eb95b38a320fdb\n",
            "Reusing dataset image_folder (/scratch/cs344/huggingface/datasets/image_folder/default-73eb95b38a320fdb/0.0.0/ee92df8e96c6907f3c851a987be3fd03d4b93b247e727b69a8e23ac94392a091)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71d3c2edbff74b3cae20d0d3f4785f1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/27 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-a28225d917e12e11\n",
            "Reusing dataset image_folder (/scratch/cs344/huggingface/datasets/image_folder/default-a28225d917e12e11/0.0.0/ee92df8e96c6907f3c851a987be3fd03d4b93b247e727b69a8e23ac94392a091)\n"
          ]
        }
      ],
      "source": [
        "tr_p = '/home/tt35/Desktop/pics_transformers/train'\n",
        "te_p = '/home/tt35/Desktop/pics_transformers/test'\n",
        "va_p = '/home/tt35/Desktop/pics_transformers/validation'\n",
        "ds = create_ds(tr_p, te_p, va_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce0d1e5",
      "metadata": {
        "id": "6ce0d1e5"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a863d83c",
      "metadata": {
        "id": "a863d83c"
      },
      "source": [
        "I wrote the function that creates a model. It takes a dataset and model name. Some examples of the possible model name can be found [here](https://huggingface.co/models?other=vit). However, it should be noted that some models in the website are not compatible to the function. It is recommended to use [vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) as a default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1097a8a2",
      "metadata": {
        "id": "1097a8a2"
      },
      "outputs": [],
      "source": [
        "def create_model(ds, model_name_or_path = 'google/vit-base-patch16-224-in21k'):\n",
        "        mn = model_name_or_path\n",
        "        labels = ds['train'].features['label'].names\n",
        "        model = ViTForImageClassification.from_pretrained(\n",
        "        mn,\n",
        "        num_labels=len(labels),\n",
        "        id2label={str(i): c for i, c in enumerate(labels)},\n",
        "        label2id={c: str(i) for i, c in enumerate(labels)}\n",
        "        )\n",
        "        \n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b80854aa",
      "metadata": {
        "id": "b80854aa",
        "outputId": "bff500bf-c349-45cc-be01-63797b7c7f64"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = create_model(ds, model_name_or_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a89139d5",
      "metadata": {
        "id": "a89139d5"
      },
      "source": [
        "### Make a Trainer "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac50b2f",
      "metadata": {
        "id": "fac50b2f"
      },
      "source": [
        "Unlike #1, this function let you choose various parameters such as batch_size, and epohcs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f873b0",
      "metadata": {
        "id": "75f873b0"
      },
      "outputs": [],
      "source": [
        "def make_trainer(model, batch_size=8, epochs=3, learning_rate=2e-4, save_steps=100, eval_steps=100, logging_steps=10, save_total_limit=2):\n",
        "    prepared_ds = ds.with_transform(transform)\n",
        "    trainer1 = Trainer(\n",
        "        model=model,\n",
        "        args=TrainingArguments(\n",
        "          output_dir=\"./vit-base\",\n",
        "          per_device_train_batch_size=batch_size,\n",
        "          evaluation_strategy=\"steps\",\n",
        "          num_train_epochs=epochs,\n",
        "          fp16=True,\n",
        "          save_steps=save_steps,\n",
        "          eval_steps=eval_steps,\n",
        "          logging_steps=logging_steps,\n",
        "          learning_rate=learning_rate,\n",
        "          save_total_limit=save_total_limit,\n",
        "          remove_unused_columns=False,\n",
        "          push_to_hub=False,\n",
        "          report_to='tensorboard',\n",
        "          load_best_model_at_end=True,\n",
        "        ),\n",
        "        data_collator=collate_fn,\n",
        "        compute_metrics=compute_metrics,\n",
        "        train_dataset=prepared_ds[\"train\"],\n",
        "        eval_dataset=prepared_ds[\"validation\"],\n",
        "        tokenizer=feature_extractor,\n",
        "    )\n",
        "    \n",
        "    return trainer1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbecec69",
      "metadata": {
        "id": "cbecec69"
      },
      "outputs": [],
      "source": [
        "trainer = make_trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025f8511",
      "metadata": {
        "id": "025f8511",
        "outputId": "850d21e8-dd3b-40ba-b2dd-0615a83645b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "trainer = make_trainer(model, batch_size=8, epochs=3, learning_rate=2e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "673307ca",
      "metadata": {
        "id": "673307ca"
      },
      "source": [
        "### Model Testing and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91f3203c",
      "metadata": {
        "id": "91f3203c"
      },
      "source": [
        "Model testing and evaluation are done in the same way as #1 as I believe it does not require intensive coding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6c9cb4a",
      "metadata": {
        "id": "d6c9cb4a",
        "outputId": "b98ba2f4-f536-43e5-ee0d-296e956aa140"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tt35/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 107\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 42\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [42/42 00:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Saving model checkpoint to ./vit-base\n",
            "Configuration saved in ./vit-base/config.json\n",
            "Model weights saved in ./vit-base/pytorch_model.bin\n",
            "Feature extractor saved in ./vit-base/preprocessor_config.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 23166582GF\n",
            "  train_loss               =     0.1561\n",
            "  train_runtime            = 0:00:31.35\n",
            "  train_samples_per_second =     10.239\n",
            "  train_steps_per_second   =       1.34\n"
          ]
        }
      ],
      "source": [
        "random.seed(100)\n",
        "train_results = trainer.train()\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67be9d14",
      "metadata": {
        "id": "67be9d14"
      },
      "outputs": [],
      "source": [
        "metrics = trainer.evaluate(prepared_ds['test'])\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Final_Project_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}